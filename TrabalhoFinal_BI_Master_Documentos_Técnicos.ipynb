{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrabalhoFinal-BI-Master_Documentos_Técnicos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jedias/BI-Master/blob/main/TrabalhoFinal_BI_Master_Documentos_T%C3%A9cnicos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZKGdsebyT09"
      },
      "source": [
        "#Introdução\n",
        "O procesasmento de linguagem natural através dos metodos de IA é um problema que tem recebido bastante atenção nos ultimos anos. Sua aplicação abrange desde a construção de chatterbots até tradução de texto. \n",
        "Esse trabalho busca demonstrar a viabilidade da geração automática de documentos a partir de tabelas factuais. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGdFUyGPx_cO"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AugoIQTmzVHE"
      },
      "source": [
        "#Aquisição dos dados\n",
        "\n",
        "Inicalmente é necessária a escolha de um corpus de referência. Nesse caso o corpus escolhido será o conjunto de normas técnicas disponível em https://canalfornecedor.petrobras.com.br/pt/regras-de-contratacao/catalogo-de-padronizacao/#especificacoes-tecnicas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swRSUhg8Spqj"
      },
      "source": [
        "Parsing os documentos PDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2H3aIXYUZid"
      },
      "source": [
        "!pip install PyPDF2\n",
        " \n",
        "#importando as bilibliotecas usadas no parsing\n",
        "\n",
        "import bz2\n",
        "import zipfile\n",
        "import PyPDF2\n",
        "from glob2 import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9jiSsZhKje7"
      },
      "source": [
        "def pdf2pandas(local):\n",
        "    docn = 0\n",
        "    df = pd.DataFrame({'item':[] ,'text':[]})\n",
        "    filelist = glob(local+'/**/*.PDF',recursive=True)\n",
        "    \n",
        "    for item in filelist:\n",
        "      \n",
        "      with open(item,'rb') as pdf_file:\n",
        "        read_pdf = PyPDF2.PdfFileReader(item)\n",
        "        number_of_pages = read_pdf.getNumPages()\n",
        "        docn = docn + 1\n",
        "        docx = ''\n",
        "        \n",
        "        for page_number in range(number_of_pages):  \n",
        "          page = read_pdf.getPage(page_number)\n",
        "          page_content = page.extractText()\n",
        "          docx = docx + page_content \n",
        "            \n",
        "        df.loc[docn,'text'] = docx\n",
        "        df.loc[docn,'item'] = item\n",
        "        print(df.tail(1))\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VKzsKKmacoK"
      },
      "source": [
        "npt = pdf2pandas('/content/drive/My Drive/Colab Notebooks/TrabalhoBi/normas/')\n",
        "npt.to_csv('/content/drive/My Drive/Colab Notebooks/TrabalhoBi/normas/ptbr.csv') \n",
        "nin = pdf2pandas('/content/drive/My Drive/Colab Notebooks/TrabalhoBi/NI/')\n",
        "nin.to_csv('/content/drive/My Drive/Colab Notebooks/TrabalhoBi/NI/english.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbAnlgWpIb03"
      },
      "source": [
        "Após o parsing dos documentos, eles foram salvos em CSV. Essa passo economiza tempo na retomada desses textos no futuro. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4cOPopKJFX6"
      },
      "source": [
        "npt = pd.read_csv('/content/drive/My Drive/Colab Notebooks/TrabalhoBi/normas/ptbr.csv')\n",
        "npt.drop('Unnamed: 0', axis='columns', inplace=True)\n",
        "nin = pd.read_csv('/content/drive/My Drive/Colab Notebooks/TrabalhoBi/NI/english.csv')\n",
        "nin.drop('Unnamed: 0', axis='columns', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj2whvQeICsl"
      },
      "source": [
        "npt.item.replace({'/content/drive/My Drive/Colab Notebooks/TrabalhoBi/normas/':''}, regex=True, inplace = True)\n",
        "nin.item.replace({'/content/drive/My Drive/Colab Notebooks/TrabalhoBi/NI/':''}, regex=True, inplace = True)\n",
        "nin.item.replace({'I':''}, regex=True, inplace = True)\n",
        "corpus = npt.merge(nin, how='outer', left_on='item', right_on='item')\n",
        "corpus.dropna(inplace = True)\n",
        "corpus['text_x'] = corpus['text_x'].str.lower()\n",
        "corpus['text_y'] = corpus['text_y'].str.lower()\n",
        "corpus.to_excel('/content/drive/My Drive/Colab Notebooks/TrabalhoBi/corpus.xlsx')\n",
        "corpus.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raqq4zLmUV0k"
      },
      "source": [
        "Continuando..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyiRU3DYbkpc"
      },
      "source": [
        "corpus = pd.read_excel('/content/drive/My Drive/Colab Notebooks/TrabalhoBi/corpus.xlsx')\n",
        "corpus.drop('Unnamed: 0', axis='columns', inplace=True)\n",
        "corpus.replace({'\\n':' '}, regex=True, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-HmQX5TGToD"
      },
      "source": [
        "corpus.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwnYmLEoEaoe"
      },
      "source": [
        "# Modelamento de linguagem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5br-dvwjEaoi"
      },
      "source": [
        "# 1. Pre-processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NABWXet8Eaoi"
      },
      "source": [
        "# Tokenização do texto\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXiw7ea3Eaof"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJjeYfq0Eaoi"
      },
      "source": [
        "#mantendo documentos individuais na sua coluna\n",
        "corpus['Xsent'] = corpus.text_x.apply(lambda row: nltk.sent_tokenize(row))\n",
        "corpus['Ysent'] = corpus.text_y.apply(lambda row: nltk.sent_tokenize(row))\n",
        "corpus['Xtk'] = corpus.text_x.apply(lambda row: nltk.word_tokenize(row, language = 'portuguese'))\n",
        "corpus['Ytk'] = corpus.text_y.apply(lambda row: nltk.word_tokenize(row, language = 'english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X0HSruqFpyN"
      },
      "source": [
        "corpus.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V57nHZz5Eaon"
      },
      "source": [
        "#Treinamento dos Word2Vec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUsji8kb1PKl"
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0OSI9En2vOh"
      },
      "source": [
        "modelX = Word2Vec(sentences = corpus.Xtk, size = 300, window = 50, min_count=1, workers=100)\n",
        "modelY  = Word2Vec(sentences = corpus.Ytk, size = 300, window = 50, min_count = 1, workers = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6Dk__qUYUMN"
      },
      "source": [
        "modelX.save(\"/content/drive/My Drive/word2vec.pt.model\")\n",
        "modelY.save(\"/content/drive/My Drive/word2vec.en.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5QKshN-ySit"
      },
      "source": [
        "modelX = Word2Vec.load(\"/content/drive/My Drive/word2vec.pt.model\")\n",
        "modelY =  Word2Vec.load(\"/content/drive/My Drive/word2vec.en.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzryqTtpZtw1"
      },
      "source": [
        "# Conhecendo os dados\n",
        "\n",
        "[Fonte](https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m3AcTt5Gu-4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "from gensim.models import Doc2Vec\n",
        "from sklearn import utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gensim\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import re\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H7HjJvlKpXQ"
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOOUvnxdHgUb"
      },
      "source": [
        "def display_pca_scatterplot(model, words=None, sample=0):\n",
        "    if words == None:\n",
        "        if sample > 0:\n",
        "            words = np.random.choice(list(model.wv.vocab.keys()), sample)\n",
        "        else:\n",
        "            words = [ word for word in model.wv.vocab ]\n",
        "        \n",
        "    word_vectors = np.array([model.wv[w] for w in words])\n",
        "\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "    \n",
        "    plt.figure(figsize=(24,16))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyELzTQ_2Zui"
      },
      "source": [
        "display_pca_scatterplot(model = modelX, words = modelX.wv.vocab.keys(),sample = 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqR6baJCKWX8"
      },
      "source": [
        "display_pca_scatterplot(modelX,['bomba','compressor','vaso','manutenção','operação','solda','selo','sobressalente','confiabilidade'] )\n",
        "display_pca_scatterplot(modelY,['pump','compressor','vessel','maintenance','operation','weld','seal','extra','spare','reliability'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMx8m8DO4A8E"
      },
      "source": [
        "display_pca_scatterplot(modelX,['n-1959','n-2201','n-2508','n-0898','bomba','compressor'])\n",
        "display_pca_scatterplot(modelX,['n-1959','n-2201','n-2508','n-0898','pump', 'compressor'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jjk0T-oJATo"
      },
      "source": [
        "modelX.wv.similar_by_word('petrobras')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC7WiIbREdW7"
      },
      "source": [
        "modelX.wv.similar_by_vector(modelX.wv['petrobras']-modelX.wv['produção'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfI0lnd1Ic08"
      },
      "source": [
        "modelX.wv.similar_by_vector(modelX.wv['petrobras']-modelX.wv['óleo']-modelX.wv['gás']-modelX['produção'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVXi1S7kGV5G"
      },
      "source": [
        "modelX.wv.similar_by_vector(modelX.wv['confiabilidade']+modelX.wv['manutenção'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHqKlLUxFh_b"
      },
      "source": [
        "modelX.wv.similar_by_vector(modelX.wv['confiabilidade']-modelX.wv['sobressalente'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eda4JNoaHZaP"
      },
      "source": [
        "modelX.most_similar(positive = ['bomba','confiabilidade'],negative = 'spare')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueAex_7wtsqA"
      },
      "source": [
        "# Tradução W2V"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G7ua5AQEgES"
      },
      "source": [
        "from gensim.models.translation_matrix import TranslationMatrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VDOAGAdoJn4"
      },
      "source": [
        "model_trans = TranslationMatrix(modelX.wv, modelY.wv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MFHErfA_9v-"
      },
      "source": [
        "#lista de palavras\n",
        "word_pairs = [('petrobras','petrobras'), ('inspeção','inspection'),\n",
        "              ('elétrica','electrical'),('sistema','system'), \n",
        "              ('responsável','responsible'),\n",
        "              ('motor','motor'),('elétrico','electric'),\n",
        "              ('solda','welding'),\n",
        "              ('cabo','cable'), ('fio','wire'),\n",
        "              ('tubo','pipe'),\n",
        "              ('superfície','surface'),('norte','north'),\n",
        "              ('número','number'), ('dimensão','dimension'),\n",
        "              ('mínimos','minimum'),\n",
        "              ('mm','mm'), ('m³','m³'),\n",
        "              ('secundário', 'secondary'),\n",
        "              ('compressor','compressor'),('compressores','compressors'),\n",
        "              ('são','are'), ('é','is'), ('podem','may'),\n",
        "              ('petróleo','oil'), ('combustível','fuel'),\n",
        "              ('o','the'), ('mesmas','same'),\n",
        "              ('do','of'),\n",
        "              ('maior','larger'),('detalhes','details'),\n",
        "              ('para','for'),('com','with'),\n",
        "              ('(',\"(\"), (',',\",\"),('caldeira','boiler'),\n",
        "              ('nota','note'),('todos','all'),('todas','all'),\n",
        "              ('mesmo','same'), ('verde','green'),('azul','blue'),\n",
        "              ('água','water'),('óleo','oil'), ('substitui','replaces'),\n",
        "              ('apresentados','presented'), ('tendo','having'),\n",
        "              ('temporários','temporary'), ('desenhos','drawings'), \n",
        "              ('seqüências','sequences'),('seqüência','sequence'),\n",
        "              ('previstas','foreseen'), ('volume','volume'),\n",
        "              (\"/\",'/'), \n",
        "              ('n-2201','n-2201'),\n",
        "              ('propostas','proposals'), ('tabelas','tables'),\n",
        "              ('um', 'a'), ('uma','a'), ('esta','this'),('norma','standard'),\n",
        "              ('contém','contains'), \n",
        "              ('requisitos','requirements'), (\"e\",'and'), \n",
        "              ('recomendadas','recommended'), ('práticas','practices'),\n",
        "              ('padrão','standard'),('norma','normative'),('incluem',\"include\"),\n",
        "              ('anexos','annexes'),('nota','note'),\n",
        "              ('localização','location'),\n",
        "              ('autora','author'),('similares','similar'),\n",
        "              ('existe','is'),\n",
        "              ('detalhamento','detailing'),\n",
        "              ('projetos','projects'),\n",
        "              ('página','page'),('páginas','pages'),\n",
        "              ('apresentação','presentation'), ('carga','cargo'),\n",
        "              ('fases',\"phases\"),('cimento','cement'),('água/cimento','water/cement'),\n",
        "              ('acabamento','finishing'),('superfícies','surfaces'),\n",
        "              ('4.1.5','4.1.5'), ('8.2.2.2','8.2.2.2'),('4.6.2.3','4.6.2.3'),\n",
        "              ('4.6.2.3','4.6.2.3'), \n",
        "              ('estruturas','structures'),('estrutural','structural'),\n",
        "              ('ressonância','resonance'), ('defensa','shield'),\n",
        "              ('concreto','concrete'),('estão','are'),('aço','steel'),\n",
        "              ('pelo','by')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8My65byhB1F"
      },
      "source": [
        "model_trans.train(word_pairs=word_pairs)\n",
        "model_trans.translate(['tubulação','bomba'],topn=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmNg_nhwAUv-"
      },
      "source": [
        "model_trans.translate(corpus.Xtk[0][50:150],topn=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH8oEeUGP1fF"
      },
      "source": [
        "#infelizmente os resultados são ruins"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G808kv3L_GVM"
      },
      "source": [
        "\n",
        "# Buscas **W2V**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rodD-IJu_FxV"
      },
      "source": [
        "modelX.most_similar(positive = ['manutenção','bomba'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NetesWr_-wk"
      },
      "source": [
        "modelY.most_similar(positive = ['pump','maintenance'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9KH8fmCCL9S"
      },
      "source": [
        "Mesmo partindo da tradução manual dos documentos, os contextos em cada lígua não idênticos.\n",
        "Isso pode indicativo de desintendimentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbi4giyVi-9m"
      },
      "source": [
        "# Modelo GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QM8dqo9bkAnI"
      },
      "source": [
        "#https://radimrehurek.com/gensim/scripts/glove2word2vec.html\n",
        "\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48O-1Rr7AUu1"
      },
      "source": [
        "import gensim.downloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kiERouAAWiF"
      },
      "source": [
        "#testando GloVe 300d sem treino\n",
        "# Download the \"glove\" embeddings\n",
        "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-300')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdZ2-t0uCWUe"
      },
      "source": [
        "# Use the downloaded vectors as usual:\n",
        "glove_vectors.most_similar('petrobras')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD1ZfMqRrpjl"
      },
      "source": [
        "glove_vectors.most_similar(positive ='maintenance')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiQ6GYXAA4df"
      },
      "source": [
        "glove_vectors.most_similar(positive =['maintenance', 'petrobras'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaYfCRd8wC2E"
      },
      "source": [
        "glove_vectors.most_similar(positive = ['pump','reliability'],negative = ['spare'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAl9kezDCE5o"
      },
      "source": [
        "#pedidos da Zelda\n",
        "glove_vectors.most_similar(positive = ['cute','cat'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xP3yRd4pDZ0Y"
      },
      "source": [
        "glove_vectors.most_similar(positive = ['reptile','venomous'],negative = ['limbs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1kLtu3bD2ik"
      },
      "source": [
        "glove_vectors.most_similar(positive = ['gray','mammal','africa','asia','trunk','animal'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN3jJF8gDdgY"
      },
      "source": [
        "glove_vectors.train(corpus.tkY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ItQvhrmF6WT"
      },
      "source": [
        "Infelizemente o modelo GloVe pré treinado não a continuaçao do treinamento com nossos documentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W28kirxGNcw"
      },
      "source": [
        "Mas isso pode ser contornado:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOdUVXcSPM6r"
      },
      "source": [
        "#Modelo GloVe + Normas\n",
        "glove_file = '/content/drive/My Drive/glove.6B.50d.txt'\n",
        "tmp_file = '/content/drive/My Drive/Colab Notebooks/TrabalhoBi/normas/english.csv'\n",
        "_ = glove2word2vec(glove_file, tmp_file)\n",
        "\n",
        "gmodel = KeyedVectors.load_word2vec_format(tmp_file)\n",
        "#gmodel.save('/content/drive/My Drive/word2glovec.en.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_B0hwqtys8s"
      },
      "source": [
        "gmodel= KeyedVectors.load('/content/drive/My Drive/word2glovec.en.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVZq8cU7slZz"
      },
      "source": [
        "gmodel.most_similar('petrobras')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l4wnQ1ttAxT"
      },
      "source": [
        "gmodel.wv.similar_by_vector(gmodel.wv['pizza']-gmodel.wv['filling'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xs-lPZvxvXK5"
      },
      "source": [
        "gmodel.wv.words_closer_than('reliability','maintainability')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hj-n7t9sth6"
      },
      "source": [
        "gmodel.similar_by_word('reliability')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dZhIVd8uYqJ"
      },
      "source": [
        "gmodel.words_closer_than('reliability','maintainability')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok-rSzVvut1e"
      },
      "source": [
        "# Tradução GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPQ4sqEmE8Iq"
      },
      "source": [
        "from gensim.models.translation_matrix import TranslationMatrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UJEtFNPDUt9"
      },
      "source": [
        "#foram excluídas as palavras não reconhecidas pelo GloVe\n",
        "\n",
        "word_pairs = [('petrobras','petrobras'), ('propriedade','property'),\n",
        "              ('substitui','replaces'),('cancela','cancels'),\n",
        "              ('inspeção','inspection'),\n",
        "              ('elétricas','electrical'),('sistema','system'), \n",
        "              ('motor','motor'),('elétrico','electric'),\n",
        "              ('solda','welding'), ('cabo','cable'),\n",
        "              ('expostas','exposed'),\n",
        "              ('bomba','pump'), ('tubo','pipe'),\n",
        "              ('superfície','surface'),('norte','north'),\n",
        "              ('número','number'), ('dimensão','dimension'),\n",
        "              ('mínimos','minimum'),\n",
        "              ('mm','mm'), \n",
        "              ('secundário', 'secondary'),\n",
        "              ('compressor','compressor'),\n",
        "              ('são','are'), ('é','is'), ('podem','may'),\n",
        "              ('petróleo','oil'), ('combustível','fuel'),\n",
        "              ('o','the'), ('mesmas','same'),\n",
        "              ('de','of'),('do','of'),('da','of'),\n",
        "              ('maior','larger'),('detalhes','details'),\n",
        "              ('para','for'),('com','with'),\n",
        "              ('caldeira','boiler'),\n",
        "              ('nota','note'),('todos','all'),('todas','all'),\n",
        "              ('mesmo','same'), ('verde','green'),('azul','blue'),\n",
        "              ('água','water'),('óleo','oil'), ('substitui','replaces'),\n",
        "              ('apresentados','presented'), ('tendo','having'),\n",
        "              ('temporários','temporary'),\n",
        "              ('desenhos','drawings'), ('seqüências','sequences'),\n",
        "              ('previstas','foreseen'), \n",
        "              (\"/\",'/'), ('(',\"(\"), (',',\",\"),(\")\",\")\"),('.','.'),\n",
        "              ('propostas','proposals'), ('tabelas','tables'),\n",
        "              ('um', 'a'), ('uma','a'), ('esta','this'),('norma','standard'),\n",
        "              ('contém','contains'), ('técnicos','technical'),\n",
        "              ('requisitos','requirements'), (\"e\",'and'), \n",
        "              ('recomendadas','recommended'), ('práticas','practices'),\n",
        "              ('padrão','standard'),('norma','normative'),('incluem',\"include\"),\n",
        "              ('anexos','annexes'),('nota','note'),\n",
        "              ('localização','location'),\n",
        "              ('autora','author'),('similares','similar'),\n",
        "              ('içamentos','lifting'), ('existe','is'),\n",
        "              ('travejamentos','supporting'), ('detalhamento','detailing'),\n",
        "              ('fases',\"phases\"),('cimento','cement'),\n",
        "              ('acabamento','finishing'),('superfícies','surfaces'),\n",
        "              ('ressonância','resonance'),('por','by'),('projetos','projects'),\n",
        "              ('página','page'),('páginas','pages'),('apresentação','presentation'),\n",
        "              ('carga','cargo'),('1','1'),('concreto','concrete'),\n",
        "              ('anterior','previous'),('à','to'),('desta','this')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY_RDsxTWz-d"
      },
      "source": [
        "model_trans_glove = TranslationMatrix(modelX.wv,gmodel)\n",
        "model_trans_glove.train(word_pairs=word_pairs)\n",
        "model_trans_glove.save(\"/content/drive/My Drive/glove.doc2vec.x.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jlr4QMdqtJHw"
      },
      "source": [
        "model_trans_glove = TranslationMatrix.load(\"/content/drive/My Drive/glove.doc2vec.x.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVB28m8pY51X"
      },
      "source": [
        "#tradução palavra a palavra\n",
        "for doc in range(0):\n",
        "  for word in range(len(corpus.Xtk[doc])):\n",
        "      try:  print(model_trans_glove.translate(corpus.Xtk[doc],topn=3))\n",
        "      except:  print(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHov-JBSWhUz"
      },
      "source": [
        "model_trans_glove.translate(corpus.Xtk[0][50:150],topn=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW6XpKMM1JuV"
      },
      "source": [
        "... um pouco melhor mas ainda insuficiente. Poderíamos continuar acrescentando sinônimos mas isso consome tempo. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buHDGZ7PTFxq"
      },
      "source": [
        "## Geraçao de glossário: Ingles técnico - GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7wuM-ViNivz"
      },
      "source": [
        "#gerando lista de palavras conhecidas\n",
        "word_pairs1 = []\n",
        "for line in range(len(word_pairs)):\n",
        "  word_pairs1 = word_pairs1 + [(word_pairs[line][1],word_pairs[line][1])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVdfyzVj2Kh7"
      },
      "source": [
        "word_pairs1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNpmF9N3Kaid"
      },
      "source": [
        "model_trans_glove = TranslationMatrix(modelY.wv,glove_vectors)\n",
        "model_trans_glove.train(word_pairs=word_pairs1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Lm4PfkwG-w3"
      },
      "source": [
        "model_trans_glove.translate([\"pipe\",'pump','oil','pressure'],topn=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2IlhjGBNkuW"
      },
      "source": [
        "for doc in range(len(corpus)):\n",
        "    for word in corpus.Ytk[doc]:\n",
        "      try: print(model_trans_glove.translate(word,topn=3))\n",
        "      except: print(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqvg4Ygv8px-"
      },
      "source": [
        "Por esse método conseguimos obter alguns sinônimos mas ainda com desempenho insatisfatório."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bnSxKT0AodD"
      },
      "source": [
        "## Gensim - Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzzrrs5WeVMY"
      },
      "source": [
        "#https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5\n",
        "#Import all the dependencies\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpQVVPcSF-yF"
      },
      "source": [
        "data = corpus.item + corpus.text_x\n",
        "\n",
        "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
        "\n",
        "max_epochs = 100\n",
        "vec_size = 300\n",
        "alpha = 0.025\n",
        "\n",
        "modelo = Doc2Vec(vector_size=vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                dm =1)\n",
        "  \n",
        "modelo.build_vocab(tagged_data)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print('iteration {0}'.format(epoch))\n",
        "    modelo.train(tagged_data,\n",
        "                total_examples=modelo.corpus_count,\n",
        "                epochs=modelo.epochs)\n",
        "    # decrease the learning rate\n",
        "    modelo.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    modelo.min_alpha = modelo.alpha\n",
        "\n",
        "modelo.save(\"/content/drive/My Drive/pt.d2v.model\")\n",
        "print(\"Model Saved\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RViYO-naQI6V"
      },
      "source": [
        "data = corpus.item + corpus.text_y\n",
        "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
        "\n",
        "max_epochs = 100\n",
        "vec_size = 300\n",
        "alpha = 0.025\n",
        "\n",
        "model = Doc2Vec(vector_size=vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                dm =1)\n",
        "  \n",
        "model.build_vocab(tagged_data)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print('iteration {0}'.format(epoch))\n",
        "    model.train(tagged_data,\n",
        "                total_examples=model.corpus_count,\n",
        "                epochs=model.epochs)\n",
        "    # decrease the learning rate\n",
        "    model.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    model.min_alpha = model.alpha\n",
        "\n",
        "model.save(\"/content/drive/My Drive/en.d2v.model\")\n",
        "print(\"Model Saved\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbFAyMh6ujGP"
      },
      "source": [
        "#busca de sentenças\n",
        "new_sentence = 'confiabilidade de equipamentos'\n",
        "modelo.docvecs.most_similar([modelo.infer_vector(new_sentence)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2D3E6a3y3cF"
      },
      "source": [
        "corpus.text_x[118][100:150]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujTEV6bMukuD"
      },
      "source": [
        "## Tradução Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4ntH4ZJI1O9"
      },
      "source": [
        "# Continuando\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "\n",
        "modelo= Doc2Vec.load(\"/content/drive/My Drive/pt.d2v.model\")\n",
        "model= Doc2Vec.load(\"/content/drive/My Drive/en.d2v.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi7OHrQrT2Wz"
      },
      "source": [
        "from gensim.models.translation_matrix import TranslationMatrix\n",
        "model_trans = TranslationMatrix(modelo.wv, model.wv)\n",
        "model_trans.train(word_pairs=word_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URMewdN7ifBl"
      },
      "source": [
        "model_trans.save(\"/content/drive/My Drive/doc2vec.x.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcIcrbdvkByq"
      },
      "source": [
        "model_trans.translate(['temperatura','petrobras','norma','tubulação','água'],topn=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10WEnJfl3te9"
      },
      "source": [
        "Notamos um desepenho aparentemente pior para essa aplicação quando comparado ao modelo W2V"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMlxQt4Gr5m2"
      },
      "source": [
        "modelo.wv.similar_by_word('bomba')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRmQKi3suPS9"
      },
      "source": [
        "modelo.wv.similarity('bomba','voluta')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vng7HqHQEaos"
      },
      "source": [
        "#Treinamento da Tradução"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAs_WOi_G9pq"
      },
      "source": [
        "!pip install keras_tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi1a-Jtg5yDY"
      },
      "source": [
        "from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D\n",
        "from keras.layers import Input\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM , GRU, SimpleRNN\n",
        "from keras.models import Sequential, Model\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "from keras_tqdm import TQDMNotebookCallback\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCBkX4PW79At"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTil8FvSbS6B"
      },
      "source": [
        "train = corpus[:10]\n",
        "train['Xvec'] = train.Ytk.apply(lambda row: modelY.wv[row])\n",
        "#train['gvec'] = train.tken.apply(lambda row: gmodel.wv[row])  #falha por palavras desconhecidas\n",
        "train['Yvec'] = train.Xtk.apply(lambda row: modelX.wv[row])\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFqYQSBNE1WZ"
      },
      "source": [
        "##LSTM \n",
        "input considerando todo o documento como um arranjo de vetores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5PhUSWG5LJv"
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras import regularizers, optimizers \n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Dropout, Embedding, Flatten, Bidirectional, Input, LSTM, Reshape\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import categorical_accuracy, mean_squared_error, mean_absolute_error\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk9d8r03DWwT"
      },
      "source": [
        "X = np.array(train.Xvec)\n",
        "Y = np.array(train.Yvec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOc7KxjM7dws"
      },
      "source": [
        "from tensorflow.keras import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VxXHAF8NYfH"
      },
      "source": [
        "paddedX = sequence.pad_sequences(X)\n",
        "paddedY = sequence.pad_sequences(Y)\n",
        "X = np.expand_dims(paddedX, axis = np.zeros(300))\n",
        "print('Input Shape: ', X.shape) \n",
        "Y = np.expand_dims(paddedY, axis = np.zeros(300))\n",
        "print('Output Shape', Y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xpeiUe4-Tfj"
      },
      "source": [
        "from keras.models import Sequential  \n",
        "from keras.layers import *  \n",
        "\n",
        "IAmodel = Sequential(\n",
        "    [\n",
        "        Dense(2, activation=\"relu\", name=\"layer1\"),\n",
        "        Dense(3, activation=\"relu\", name=\"layer2\"),\n",
        "        Dense(4, name=\"layer3\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "IAmodel.add(Reshape(Y.shape))\n",
        "IAmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizers.SGD(lr=0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re_w9ajuHzm4"
      },
      "source": [
        "tradutor = IAmodel.fit(X, Y, batch_size = 1, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ8coviZOeRb"
      },
      "source": [
        "As dimensões de entrada e saída variam do uma língua para outra, inviabilizando o uso do modelo sequencial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMHuSdyOyADq"
      },
      "source": [
        "## Enconder/Decoder LSTM\n",
        "Material de aula +\n",
        "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7Hp-i38_Uou"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl_3YMBk_O2s"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5_vT9zv_LwT"
      },
      "source": [
        "corpus = pd.read_excel('/content/drive/My Drive/Colab Notebooks/TrabalhoBi/corpus.xlsx')\n",
        "corpus.drop('Unnamed: 0', axis='columns', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAa6KwBkM6m4"
      },
      "source": [
        "corpus = corpus.replace(to_replace= r'*\\n*',value='') #nao funcionou nao sei pq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6ya4Cj4_E0j"
      },
      "source": [
        "#mantendo documentos individuais na sua coluna\n",
        "X = corpus.text_x.apply(lambda row: nltk.sent_tokenize(row,language = 'portuguese'))\n",
        "Y = corpus.text_y.apply(lambda row: nltk.sent_tokenize(row,language = 'english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uiYOyg0DUJ2"
      },
      "source": [
        "X = pd.DataFrame(X.to_list())\n",
        "Y = pd.DataFrame(Y.to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fP4ZHmxv89a"
      },
      "source": [
        "def AddTags(sentence):\n",
        "  return 'sent/  ' + sentence+ ' /sent'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_9reLu1B4M-"
      },
      "source": [
        "X = X.apply(lambda sent: AddTags(sent))\n",
        "Y = Y.apply(lambda sent: AddTags(sent))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "000-HbgC8FMb"
      },
      "source": [
        "X.fillna(\" \",inplace=True)\n",
        "Y.fillna(\" \",inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvCkWP5_Lzkx"
      },
      "source": [
        "A = X.apply(lambda row: row.str.cat(sep=' '),axis = 1)\n",
        "\n",
        "B = Y.apply(lambda row: row.str.cat(sep=' '),axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0l4YpI7QZ7V"
      },
      "source": [
        "A.replace('\\n',' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ2BY4LsPSbt"
      },
      "source": [
        "X=A\n",
        "Y=B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XWN-1vnNE-Z"
      },
      "source": [
        "for row in range(len(X)):\n",
        "  X[row]= nltk.word_tokenize(X[row], language = 'portuguese')\n",
        "for row in range(len(Y)): \n",
        "  Y[row] = nltk.word_tokenize(Y[row], language = 'english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYaumFismjJQ"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfiKqbxSTmgh"
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gfyrIFfTeVC"
      },
      "source": [
        "mdlX = Word2Vec(sentences = X, size = 300, window = 100, min_count=1, workers=100)\n",
        "mdlY  = Word2Vec(sentences = Y, size = 300, window = 100, min_count = 1, workers = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWBFoRn1UlQs"
      },
      "source": [
        "mdlX.wv.similar_by_word('n-2201')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTC8_Ic5U-3T"
      },
      "source": [
        "mdlY.wv.similar_by_word('n-2201')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hv1Pps5T_rV"
      },
      "source": [
        "Xemb = mdlX.wv.get_keras_embedding(train_embeddings=False)\n",
        "Yemb = mdlY.wv.get_keras_embedding(train_embeddings=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nGCatTNqOCY"
      },
      "source": [
        "array_of_word_lists= X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUGxDAOnrbeg"
      },
      "source": [
        "array_of_word_lists_AFTER_being_transformed_by_word2vec = X.apply(lambda word: mdlX.wv[word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjnZjTwDoOlR"
      },
      "source": [
        "source_word_indices = []\n",
        "for i in range(len(array_of_word_lists)):\n",
        "    source_word_indices.append([])\n",
        "    for j in range(len(array_of_word_lists[i])):\n",
        "        word = array_of_word_lists[i][j]\n",
        "        if word in mdlX.wv.vocab:\n",
        "            word_index = mdlX.wv.vocab[word].index\n",
        "            source_word_indices[i].append(word_index)\n",
        "        else:\n",
        "            # Do something. For example, leave it blank or replace with padding character's index.\n",
        "            source_word_indices[i].append(padding_index)\n",
        "source = np.array(source_word_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5dwF055RO28"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, GRU, Input, Flatten, Dense, TimeDistributed, Activation, PReLU, RepeatVector, Bidirectional, Dropout\n",
        "from keras.optimizers import Adam, Adadelta\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.losses import sparse_categorical_crossentropy, mean_squared_error\n",
        "\n",
        "keras_model = Sequential()\n",
        "keras_model.add(Xemb)\n",
        "keras_model.add(Bidirectional(GRU(300, return_sequences=True, dropout=0.1, recurrent_dropout=0.1, activation='tanh')))\n",
        "keras_model.add(TimeDistributed(Dense(600, activation='tanh')))\n",
        "# keras_model.add(PReLU())\n",
        "# ^ For some reason I get error when I add Activation ‘outside’:\n",
        "# int() argument must be a string, a bytes-like object or a number, not 'NoneType'\n",
        "# But keras_model.add(Activation('relu')) works.\n",
        "#keras_model.add(Dense(source_arr.shape[1] * source_arr.shape[2]))\n",
        "# size = max-output-sentence-length * embedding-dimensions to learn the embedding vector and find the nearest word in word2vec_model.wv.similar_by_vector() afterwards.\n",
        "# Alternatively one can use Dense(vocab_size) and train the network to output one-hot categorical words instead.\n",
        "# Remember to change Keras loss to sparse_categorical_crossentropy.\n",
        "# But this won’t benefit from Word2Vec. \n",
        "\n",
        "keras_model.compile(loss=mean_squared_error,\n",
        "              optimizer=Adadelta(),\n",
        "              metrics=['mean_absolute_error'])\n",
        "keras_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdH1dsVvqrG2"
      },
      "source": [
        "filepath=\"best-weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_mean_absolute_error', verbose=1, save_best_only=True, mode='auto')\n",
        "callbacks_list = [checkpoint]\n",
        "keras_model.fit(array_of_word_lists, array_of_word_lists_AFTER_being_transformed_by_word2vec, epochs=100, batch_size=2000, shuffle=True, callbacks=callbacks_list, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rgqAyOwq0vJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncpmNoTEoElK"
      },
      "source": [
        "filepath=\"best-weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_mean_absolute_error', verbose=1, save_best_only=True, mode='auto')\n",
        "callbacks_list = [checkpoint]\n",
        "keras_model.fit(array_of_word_lists, array_of_word_lists_AFTER_being_transformed_by_word2vec, epochs=100, batch_size=2000, shuffle=True, callbacks=callbacks_list, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guy2uu806tKi"
      },
      "source": [
        "rows, columns = X.shape\n",
        "for row in range(rows):\n",
        "  for column in range(columns):\n",
        "    X.iloc[row][0] = X.iloc[row][0] + X.iloc[row][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYlcTOCp7m9v"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWZbtOUz-wou"
      },
      "source": [
        "rows, columns = X.shape\n",
        "for row in range(rows):\n",
        "  for column in range(columns):\n",
        "    X[row][0] = X[row][0] + X[row][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do-XdvIjx1wY"
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--QhJFtOyFg_"
      },
      "source": [
        "for column in X.columns:\n",
        "  modelX = Word2Vec.train(sentences = X[columns], size = 300, window = 10, min_count=2, workers=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4M_QTVc1TXD"
      },
      "source": [
        "for column in Y.columns:\n",
        "  modelY = Word2Vec.train(sentences = Y[columns])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whlJ4MXh195p"
      },
      "source": [
        "modelX.wv.similar_by_vector(np.zeros(300))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62SLkgdR19hO"
      },
      "source": [
        "modelY.wv.similar_by_vector(np.zeros(300))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-6b1XjW2nKa"
      },
      "source": [
        "len(modelX.wv.vocab.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al31kW_fvmPH"
      },
      "source": [
        "#Padding\n",
        "for col in X.columns:\n",
        "  for row in range(len(X)):\n",
        "      try:\n",
        "        X[col][row] =  modelX.wv[X[col][row]]\n",
        "      except: \n",
        "        [np.zeros(300)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC2PFv1VtRUZ"
      },
      "source": [
        "for col in Y.columns:\n",
        "  for row in range(len(Y)):\n",
        "      try:\n",
        "        Y[col][row] =  modelY.wv[Y[col][row]]\n",
        "      except: \n",
        "        [np.zeros(300)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEpSAy-xs8mf"
      },
      "source": [
        "X.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFEIsXSLyJoh"
      },
      "source": [
        "ndocs, num_encoder_tokens = X.shape\n",
        "ndocs, num_decoder_tokens = Y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezvy9lAlqFCg"
      },
      "source": [
        "num_encoder_tokens , num_decoder_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsslDBSd4V_o"
      },
      "source": [
        "latent_dim = 128\n",
        "encoder_input_data = pd.DataFrame(X)\n",
        "decoder_input_data = encoder_input_data\n",
        "decoder_target_data = pd.DataFrame(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPG1aQUdMNY-"
      },
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens)\n",
        "#encoder = LSTM(latent_dim, return_state=True)\n",
        "#encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "#encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,num_decoder_tokens))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the \n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0st0FaLe30Qt"
      },
      "source": [
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit([encoder_input_data, decoder_input_data], validation_data=decoder_input_data, batch_size=1, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tQL2tFq_jM7"
      },
      "source": [
        "modelY.wv.vocab[np.zeros(300)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCrkzV_F4DeK"
      },
      "source": [
        "fr = lines.fr.apply(lambda x : 'START_ '+ x + ' _END')\n",
        "# Create vocabulary of words\n",
        "all_eng_words=set()\n",
        "for eng in lines.eng:\n",
        "    for word in eng.split():\n",
        "        if word not in all_eng_words:\n",
        "            all_eng_words.add(word)\n",
        "    \n",
        "all_french_words=set()\n",
        "for fr in lines.fr:\n",
        "    for word in fr.split():\n",
        "        if word not in all_french_words:\n",
        "            all_french_words.add(word)\n",
        "input_words = sorted(list(all_eng_words))\n",
        "target_words = sorted(list(all_french_words))\n",
        "num_encoder_tokens = len(all_eng_words)\n",
        "num_decoder_tokens = len(all_french_words)\n",
        "# del all_eng_words, all_french_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ8Bi6h3ND7v"
      },
      "source": [
        "https://stackoverflow.com/questions/51492778/how-to-properly-use-get-keras-embedding-in-gensim-s-word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvJDSlp7AIfV"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEvJazHvAIcL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM7Hn_A5AIEh"
      },
      "source": [
        "from gensim.models import Phrases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgMkV7O0AJzM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}